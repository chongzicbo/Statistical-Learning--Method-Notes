# 3. $k$近邻法

$k$近邻法(k-nearest neighbor,$k$-NN)是一种基本分类与回归方法，通常用于解决分类问题。$k$近邻法假设给定一个训练数据集，其中的实例类别已经知道。分类时，对于新的实例，根据其$k$个最近邻的训练实例的类别，通过多数表决的方法进行预测。$k$近邻法不具有显示的学习过程，包括三个基本要素：
$k$值的选择、距离度量及分类决策规则。

## 3.1 $k$近邻算法

输入：训练数据集
$$
    T=\{(x_1,y_1),(x_2,y_2), \ldots\}
$$

其中，$x_i \in X \subseteq R^n$为实例的特征向量,$y_i \in Y= \{ x_1,c_2,\ldots,c_K\}$为实例类别，i=1,2,$\ldots,N$;

输出：实例$x$所属的类y。

（1）根据给定的距离度量，在训练集$T$中找出与$x$最近邻的$k$个点，涵盖这$k$个点的$x$的领域记作$N_k(x)$;
（2）在$N_k(x)$中根据分类决策规则决定$x$的类别$y$:
$$
y=\argmax_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j),i=1,2,\ldots,N;j=1,2,\ldots,K \qquad (3.1)
$$

上式中，$I$为指示函数，即当$y_i=c_j$时$I$为1，否则$i$为0.

$k$近邻法的特殊情况是$k=1$的情形，称为最近邻法。

## 3.2 $k$近邻模型

$k$近邻模型的三个基本要素为：距离度量、$k$值得选择和分类决策规则。

### 3.2.1 距离度量

$k$近邻模型得特征空间一般是$n$维实数向量空间$R^n$，通常使用欧氏距离衡量，更一般得距离是$L_p$距离或Minkowski距离。

假设两个实例点$x_i=(x_i^{(1)}),x_i^{(2)}),\ldots,x_i^{(n)})^T$,$x_j=(x_j^{(1)}),x_j^{(2)}),\ldots,x_j^{(n)})^T$,则$x_i$,$x_j$的$L_p$距离定义为：
$$
L_p(x_i,x_j)=(\sum _{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
$$

当$p =2$时，称为欧式距离，即：
$$
L_2(x_i,x_j)=(\sum _{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}
$$

当$p =1$时，称为曼哈顿距离，即：
$$
L_1(x_i,x_j)=(\sum _{l=1}^n|x_i^{(l)}-x_j^{(l)}|)
$$

当$p= \infty$时，它是各个坐标距离的最大值，即
$$
L_\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|
$$


### 3.2.2 $k$值得选择

&emsp;&emsp;$k$值得选择对$k$近邻法的结果具有很大的影响。
&emsp;&emsp;如果$k$值较小，相当于用较小的领域中的训练实例进行预测，则只有与输入实例较近的训练实例才对预测结果起作用，“学习”的近似误差会减小，估计误差会增大，预测结果对近邻的实例点非常敏感。$k$值的减小意味着整体模型变得复杂，容易发生过拟合。
&emsp;&emsp;如果$k$值较大，相当于用较大邻域的训练实例进行预测，可以减少估计误差，但是近似误差会增大。意味着与输入实例较远的训练实例也会起作用。$k$值的增大意味着整体的模型变得简单。

在实际训练中，一般开始选择一个较小的$k$值，然后使用交叉验证法来选取最优的$k$值。

### 3.2.3 分类决策规则

&emsp;&emsp;$k$近邻法中的分类决策规则往往是多数表决，即由输入实例的$k$个近邻的训练实例中的多数类决定输入实例的类。

## 3.3 总结

* $k$近邻法是基本且简单的分类方法。基本做法是：对给定的训练实例和输入实例点，首先确定输入实例点的$k$个最近邻训练实例点，然后利用这$k$个训练实例点的类的多数来预测输入实例点的类。
  
* $k$近邻法的三个基本要素为：距离度量、$k$值的选择和分类决策规则。常用的距离度量方法为欧式距离或者更一般的$L_p$距离。$k$值小时，模型更复杂；$k$值大时，模型更简单。通常使用交叉验证法来选择$k$值。常用的分类决策规则是多数表决。



