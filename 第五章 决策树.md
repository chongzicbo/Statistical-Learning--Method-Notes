# 第五章 决策树

&emsp;&emsp;决策树可以作为一种基本的分类放法，其模型呈树形结构，可以认为是if-then规则的集合，也可以看作是定义在特征空间与类空间上的条件概率分布。决策树模型可读性高、分类速度快。决策树学习一般包括3个基本步骤：特征选择、决策树生成和决策树修剪。决策树模型包括ID3、C4.5和CART算法。

## 5.1 决策树模型与学习

### 5.1.1 模型

&emsp;&emsp;决策树模型呈现树形结构，由结点和有向边组成。结点包括内部结点和叶结点。内部结点表示一个特征或者属性，叶结点表示一个类。使用决策树分类，就是从根结点开始，对某一实例特征进行测试，并根据测试结果将实例划分到某一子结点，递归地进行此过程，直到到达叶结点。最后获得实例对应的类别。

### 5.1.2 决策树与if-then规则

&emsp;&emsp;决策树的根结点到叶结点的每一条路径对应一条if-then规则;每条路径上内部结点的特征对应着规则的条件，叶结点的类别对应着规则的结论。每条路径都是互斥且完备的，即每一个实例都被且仅被一条路径或一条规则覆盖。

### 5.1.3 决策树与条件概率分布

&emsp;&emsp;决策树还表示给定特征条件下类的条件概率分布。将特征空间划分为互不相交的单元或区域，在每个单元定义一个类的概率分布就构成了一个条件概率分布。每一条路径对应一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$。$X$取值于给定划分下单元的集合,$Y$取值于类的集合.各叶结点上的条件概率往往偏向某一个类，决策树分类时将该结点分到条件概率最大的那一类。

### 5.1.4 决策树学习

训练数据集
$$
D=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}
$$
其中，$x_i=(x_i^{(1)},x_i^{(2)},\ldots,x_i^{(n)})$为输入实例，$n$为特征个数,$y_i \in \{1,2,\ldots,K\}$为类标记，$i=1,2,\ldots,N$,$N$为样本容量。决策树学习的目标是学习一个能够对实例进行正确分类的模型。

&emsp;&emsp;决策树学习本质是从训练数据中归纳出一组分类规则，其与训练数据矛盾较小，且具有较好的泛化能力。使用损失函数来表示这一目标，通常使用正则化的极大似然函数作为损失函数，并对其进行最小化获得模型参数。从所有可能的决策树中选择最优决策树是NP完全问题，现实中通常采用启发式方法，近似求解。

* 决策树的构建过程 <br>
（1）构建根结点，将所有训练数据放在根结点，选择最优特征，并将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类 。<br>
（2）如果子集能够被正确分类，则构建叶结点，并将这些子集分到对应的叶结点中去；如果还有子集不能被正确分类，则继续对这些子集选择新的最优特征，继续分割，构建相应结点。<br>
（3）递归地进行上述操作，直至所有子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了所属的类别。

&emsp;&emsp;使用上述方法生成的决策树可能在训练数据上分类效果较好，但是对未知测试数据未必有好的分类能力，即容易产生过拟合。因此需要对其进行剪枝，使其具有很好的泛化能力。
&emsp;&emsp;决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，剪枝则考虑全局最优。


## 5.2 特征选择

### 5.2.1 特征选择问题

&emsp;&emsp;特征选择的目的是选择对训练数据具有分类能力的特征，提高决策树学习的能力。特征选择的两个主要准则是信息增益和信息增益比。

### 5.2.2 信息增益

* 熵：表示随机变量不确定性的度量 <br>
  
设$X$是一个取有限个值的离散随机变量，其概率分布为

$$
P(X=x_i)=p_i,i=,2\ldots,n
$$
则随机变量$X$的熵定义为
$$
H(X)=- \sum_{i=1}^np_ilog_{p_i}
$$
熵只依赖于$X$的分布，而与$X$的取值无关，也可将$X$的熵记作$H(p)$。
$$
H(p)=- \sum_{i=1}^np_ilog_{p_i}
$$
熵越大，随机变量的不确定性就越大。
$$
0\leq H(p) \leq log n
$$

当随机变量只取两个值，例如1,0时，即$X$的分布为
$$
P(x=1)=p,P(X=0)=1-p, 0 \leq p \leq 1
$$
熵为：
$$
H(p)=-plog _2p-(1-p)log_2(1-p)
$$

设有随机变量$(X,Y)$，其联合概率分布为：
$$
P(X=x_i,Y=y_i)=p_{ij},i=1,2,\dots,n;j=1,2,\dots,m
$$

* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望。
$$
H(Y|X)=\sum_{i=1}^n p_iH(Y|X=x_i)
$$
其中，$p_i=P(X=x_i),i=1,2,\dots,n$。\
当熵和条件熵中概率由数据估计得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。

* 信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。
* 特征$A$对训练数据集$D$的信息增益$g(D,A)$,定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：
  $$
  g(D,A)=H(D)-H(D|A)
  $$
  也可以称之为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

  &emsp;&emsp;决策树学习应用信息增益准则选择特征。给定数据集$D$和特征$A$,经验熵$H(D)$表示对数据集$D$进行分类的不确定性，经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性。信息增益表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。信息增益依赖于特征，信息增益大的特征具有更强的分类能力。

  &emsp;&emsp;根据信息增益准则的特征选择方法是：对训练数据集D,计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

 * 信息增益算法
  输入：训练数据集$D$和特征$A$
  输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$。

  (1)计算数据集$D$的经验熵$H(D)$
  $$
  H(D)=- \sum_{k=1}^K \frac{|C_k|}{D}log_2\frac{|C_k|}{D}
  $$

  (2)计算特征$A$对数据集$D$的经验条件熵$H(D|A)$
$$
H(D|A)=\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n \frac{|D_{i}|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{D_i}log_2\frac{|D_{ik}|}{D_i}
$$

(3)计算信息增益
$$
g(D,A)=H(D)-H(D|A)
$$

&emsp;&emsp;$|D|$表示数据集样本个数。$|C_k|$表示属于类别$C_k$的样本个数，$\sum_{k=1}^K|C_k|=|D|$,$|D_i|$表示特征$A$取第$i$个值时的样本个数。$\sum_{i=1}^n|D_i|=|D|$。$|D_{ik}|$表示$D_i$中属于类$C_k$的样本集合数量。


### 5.2.3 信息增益比

&emsp;&emsp;信息增益存在偏向于选择取值较多的特征，使用信息增益比可以对这一问题进行校正。

* 信息增益比 ：特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即
  $$
    g_R(D,A)=\frac{g(D,A)}{H_A(D)}
  $$
  其中，
  $$H_A(D)=- \sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2 \frac{|D_i|}{|D|}$$


## 5.3 ID3、C4.5 、CART算法

### 5.3.1 ID3算法

输入：训练数据集$D$,特征集$A$,阈值$\epsilon$;\
输出：决策树$T$\
(1)若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$。\
(2)若$A =\empty$,则$T$为单节点树,并将$D$中实例最大的类$C_k$作为该结点的类标记，返回$T$。\
(3)否则计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；\
(4)如果$A_g$的信息增益小于阈值$\epsilon$,则置$T$为单节点树，并将$D$中实例树最大的类别$C_k$作为该结点的类标记，返回$T$。\
(5)否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例树最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\
(6)对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步(1)~步(5)，得到子树$T_i$,返回$T_i$。