# 第五章 决策树

&emsp;&emsp;决策树可以作为一种基本的分类放法，其模型呈树形结构，可以认为是if-then规则的集合，也可以看作是定义在特征空间与类空间上的条件概率分布。决策树模型可读性高、分类速度快。决策树学习一般包括3个基本步骤：特征选择、决策树生成和决策树修剪。决策树模型包括ID3、C4.5和CART算法。

## 5.1 决策树模型与学习

### 5.1.1 模型

&emsp;&emsp;决策树模型呈现树形结构，由结点和有向边组成。结点包括内部结点和叶结点。内部结点表示一个特征或者属性，叶结点表示一个类。使用决策树分类，就是从根结点开始，对某一实例特征进行测试，并根据测试结果将实例划分到某一子结点，递归地进行此过程，直到到达叶结点。最后获得实例对应的类别。

### 5.1.2 决策树与if-then规则

&emsp;&emsp;决策树的根结点到叶结点的每一条路径对应一条if-then规则;每条路径上内部结点的特征对应着规则的条件，叶结点的类别对应着规则的结论。每条路径都是互斥且完备的，即每一个实例都被且仅被一条路径或一条规则覆盖。

### 5.1.3 决策树与条件概率分布

&emsp;&emsp;决策树还表示给定特征条件下类的条件概率分布。将特征空间划分为互不相交的单元或区域，在每个单元定义一个类的概率分布就构成了一个条件概率分布。每一条路径对应一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$。$X$取值于给定划分下单元的集合,$Y$取值于类的集合.各叶结点上的条件概率往往偏向某一个类，决策树分类时将该结点分到条件概率最大的那一类。

### 5.1.4 决策树学习

训练数据集
$$
D=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}
$$
其中，$x_i=(x_i^{(1)},x_i^{(2)},\ldots,x_i^{(n)})$为输入实例，$n$为特征个数,$y_i \in \{1,2,\ldots,K\}$为类标记，$i=1,2,\ldots,N$,$N$为样本容量。决策树学习的目标是学习一个能够对实例进行正确分类的模型。

&emsp;&emsp;决策树学习本质是从训练数据中归纳出一组分类规则，其与训练数据矛盾较小，且具有较好的泛化能力。使用损失函数来表示这一目标，通常使用正则化的极大似然函数作为损失函数，并对其进行最小化获得模型参数。从所有可能的决策树中选择最优决策树是NP完全问题，现实中通常采用启发式方法，近似求解。

* 决策树的构建过程
（1）构建根结点，将所有训练数据放在根结点，选择最优特征，并将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类。
（2）如果子集能够被正确分类，则构建叶结点，并将这些子集分到对应的叶结点中去；如果还有子集不能被正确分类，则继续对这些子集选择新的最优特征，继续分割，构建相应结点。
（3）递归地进行上述操作，直至所有子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了所属的类别。

&emsp;&emsp;使用上述方法生成的决策树可能在训练数据上分类效果较好，但是对未知测试数据未必有好的分类能力，即容易产生过拟合。因此需要对其进行剪枝，使其具有很好的泛化能力。
&emsp;&emsp;决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，剪枝则考虑全局最优。





