# 2.感知机

&emsp;&emsp;感知机(perception)是一种二分类的线性分类模型，属于判别模型。感知机学习旨在找出一个分离超平面，使其能够对给定的训练数据进行线性划分。使用基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型参数。感知机的预测是利用学习得到的感知机模型对新的输入实例进行分类。

## 2.1 感知机模型

&emsp;&emsp;感知机模型由如下函数表示：
$$
f(x)=sign(w \bullet x+b) \qquad\qquad (2.1)
$$
&emsp;&emsp;其中$x \in X \subseteq R^n$表示实例的特征向量，$X$表示输入空间；$y \in Y =\{+1,-1\}$表示实例的类别，$Y$表示输出空间；$w$和$b$为感知机模型参数，$w$为权值，$b$为偏置，$w \bullet x$表示内积；$sign$是符号函数：

$$
 \left\{ \begin{array}{rcl}
+1, & x \geqslant 0 \\ -1, & x <0
\end{array}\right. \qquad\qquad (2.2)
$$

&emsp;&emsp;感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合$\{f|f(x)=w \bullet x+b \}$。

&emsp;&emsp;给定训练数据集:
$$
T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
&emsp;&emsp;其中,$x_i \in X=R^n,y_i \in Y=\{+1,-1\},i=1,2,...,N$,求得感知机模型参数$w,b$。感知机的预测是将给定的输入实例输入模型，获取其输出对应的类别。

## 2.2 感知机学习策略

### 2.2.1 线性可分数据集

&emsp;&emsp;给定训练数据集:
$$
T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
&emsp;&emsp;其中,$x_i \in X=R^n,y_i \in Y=\{+1,-1\},i=1,2,...,N$,如果存在某个超平面$S$

$$
w \bullet x +b=0
$$
能够将训练数据集中的正实例点和负实例点进行正确的划分，则称数据集$T$为线性可分数据集；否则，称数据集$T$为线性不可分数据集。

### 2.2.2 感知机学习策略

&emsp;&emsp;感知机学习的目标是求得一个超平面，使其能够将线性可分数据集中的正负实例点进行完全正确的划分。其实是需要求得感知机模型的参数$w,b$,因此需要确定一个学习策略，即定义经验损失函数并将损失函数极小化。

&emsp;&emsp;感知机模型的损失函数可以定义为误分类点的总数，但是该损失函数对于参数$w,b$不是连续可导的，且不易优化。因此，可以将损失函数定义为误分类点到分离超平面$S$的距离之和。

&emsp;&emsp;训练数据集中的任一实例点$x_0$到超平面$S$的距离为：
$$
\frac{1}{\|w\|}|w \bullet x_0+b|
$$

这里,$\|w\|$是$w$的$L_2$范数。对于误分类的实例点$(x_i,y_i)$来说，
$$
-y_i(w \bullet x_i +b)>0
$$

因此，误分类点$x_i$到超平面$S$的距离是

$$
-\frac{1}{\|w\|}y_i(w \bullet x_i+b)
$$

那么，假设误分类点集合为$M$，那么所有误分类点到超平面$S$的距离之和为：
$$
-\frac{1}{\|w\|}\sum_{x_i\in M} y_i(w \bullet x_i+b)
$$
不考虑$\frac{1}{\|w\|}$,即为感知机模型的损失函数。

给定训练数据集:
$$
T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
&emsp;&emsp;其中,$x_i \in X=R^n,y_i \in Y=\{+1,-1\},i=1,2,...,N$。感知机$sign(w \bullet x +b)$学习的的损失函数定义为
$$
L(w,b)=-\sum_{x_i \in M}y_i(w \bullet x_i+b)
$$
&emsp;&emsp;其中，$M$为误分类点的集合，这个损失函数为感知机学习的经验风险函数。如果没有误分类点，损失函数值为0.而且误分类点越少，误分类点离分离超平面越近，损失函数值就越小。
&emsp;&emsp;感知机学习的策略是在假设空间中选取使损失函数$L(w,b)$最小的模型参数$w,b$,即感知机模型。

## 2.3 感知机学习算法

感知机学习问题可以转化为求解损失函数$L(w,b)$的最优化问题，使用随机梯度下降进行求解。

$$
\min_{w,b}L(w,b)=-\sum_{x_i \in M}y_i(w \bullet x_i+b)
$$
&emsp;&emsp;其中$M$为误分类点的集合。采用随机梯度下降法极小化损失函数，求得参数$w,b$。首先选择初始化参数值$w_0,b_0$，然后使用梯度下降法来极小化目标函数。极小化过程中每次随机选择一个误分类点使其梯度下降。

&emsp;&emsp;假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由

$$
\triangledown_wL(w,b)=- \sum_{x_i \in M}y_ix_i
$$

$$
\triangledown_bL(w,b)=- \sum_{x_i \in M}y_i
$$
给出。
随机选取一个误分类点$(x_i,y_i)$，对$(w,b)$进行更新：
$$
w \leftarrow w+ \eta y_i x_i \\
b \leftarrow b+ \eta y_i
$$
式中$\eta(0<\eta\leq1)$是步长，即学习率。这样通过不断迭代使损失函数$L(w,b)$不断减小，直到为0。
