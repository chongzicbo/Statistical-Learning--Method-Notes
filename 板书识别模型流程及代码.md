  ### 1.导入所需的包

```python 
import os
import sys
import cv2
import h5py
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from time import time
from datetime import datetime
from tqdm import tqdm

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from keras.applications import inception_v3, xception, resnet50, vgg16, vgg19
from keras.applications import InceptionV3, Xception, ResNet50, VGG16, VGG19
from keras.layers import Input, Dense, Dropout, Activation, Flatten, Lambda
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from keras.models import Model
from keras.optimizers import SGD
from keras.callbacks import LearningRateScheduler
from keras.preprocessing.image import ImageDataGenerator
import keras
from keras.optimizers import Adam
from keras.utils import multi_gpu_model
from keras.callbacks import ReduceLROnPlateau

os.environ["CUDA_VISIBLE_DEVICES"] = "0,1" #这里是设置模型训练时使用第一、二块GPU
```


### 2. 参数设置
&emsp;&emsp;参数设置分为基本参数设置和超参数设置。超参数是模型无法训练的参数，需要人为设置。
```
# 基本参数配置
width, height = 224, 224  # 训练图片尺寸，不同的模型要求的图片输入不同
batch_size = 512 #超参数，表示每批次训练的样本数量，如果机器性能好，该值可适当增大
epochs = 100  # 超参数，模型迭代次数，及模型遍历数据多少轮。如果模型比较简单，该值可设置小点。
dropout_rate=0.4 #dropout主要用于防止过拟合，在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。如果模型过拟合，则可以将该值适当增大
adam = Adam() #优化器，主要用于训练过程中梯度更新。
```
    
  ### 3.数据集准备

  &emsp;&emsp;数据集准备需要把训练数据转换成模型需要的输入格式，一般是矩阵形式。模型通常分为训练集、验证集和测试集。训练集用于模型的训练，验证集用于模型训练时验证模型性能，测试集则是在模型训练完后测试模型的最终效果。

  ```
# 训练数据、验证数据和测试数据目录
train_dir = '/data/diyou/data/board/train/' #训练集
val_dir = '/data/diyou/data/board/val/' #验证集
test_dir = '/data/diyou/data/board/test/' #测试集

train_positive_dir = '/data/diyou/data/board/train/positive/'#正样本
train_negative_dir = '/data/diyou/data/board/train/negative/'#负样本

val_positive_dir = '/data/diyou/data/board/val/positive/'
val_negative_dir = '/data/diyou/data/board/val/negative/'

test_positive_dir = '/data/diyou/data/board/test/positive/'
test_negative_dir = '/data/diyou/data/board/test/negative/'

train_positive_dir_ls = os.listdir(train_positive_dir)
train_negative_dir_ls = os.listdir(train_negative_dir)

val_positive_dir_ls = os.listdir(val_positive_dir)
val_negative_dir_ls = os.listdir(val_negative_dir)

test_positive_dir_ls = os.listdir(test_positive_dir)
test_negative_dir_ls = os.listdir(test_negative_dir)

train_positive_file_count = len(train_positive_dir_ls)
train_negative_file_count = len(train_negative_dir_ls)

val_positive_file_count = len(val_positive_dir_ls)
val_negative_file_count = len(val_negative_dir_ls)

test_positive_file_count = len(test_positive_dir_ls)
test_negative_file_count = len(test_negative_dir_ls)


#在将数据集分别放在三个不同的目录后，使用flow_from_directory()方法会直接从文件夹中读取图片并转换为模型需要的输入格式
train_datagen = ImageDataGenerator()
train_generator = train_datagen.flow_from_directory(train_dir,target_size=(width, height),batch_size=batch_size, class_mode='binary')  

val_datagen = ImageDataGenerator()
val_generator = val_datagen.flow_from_directory(val_dir, target_size=(width, height), batch_size=batch_size,class_mode='binary')

test_datagen = ImageDataGenerator()
test_generator = val_datagen.flow_from_directory(test_dir, target_size=(width, height), batch_size=batch_size,class_mode='binary')

  ```


### 4.模型网络搭建
这里使用的是预训练的模型，即别人已经训练好的模型，因此只需要在其基础上做适当的调整就可以在自己的数据集上进行训练。
```
 # 模型搭建，使用VGG16预训练模型
x = Input(shape=(width, height, 3))
x = Lambda(vgg16.preprocess_input)(x)  # preprocess_input会将数据归一化到[-1,1]之间

# 基础模型，冻结所有的卷积层
base_model = VGG16(include_top=False, input_tensor=x, weights='imagenet', pooling='avg')

#由于预训练模型一般参数量比较大，我们使用预训练模型时不必训练其所有的网络层，这里只训练最后四层。
#先将所有的网络层进行冻结，即其所有参数不参与训练
for layer in base_model.layers:
    layer.trainable = False 

#然后将最后四层解冻，即最后四层的参数参与模型训练
for layer in base_model.layers[-4:]:  # 最后4层参与训练
    layer.trainable = True

#在预训练模型后可以添加自己需要的网络层。如果训练任务比较复杂，可以增加自己的网络层。
#这里任务只是二分类，所以只添加了一个Dropout层和全连接层(Dense)
y = Dropout(dropout_rate)(base_model.output)
y = Dense(1, activation='sigmoid')(y)
# Full Model: Pre-train Conv + Customized Classifier
GPU_COUNT = 2 #GPU数量

model = Model(inputs=base_model.input, outputs=y, name='VGG16')
parallel_model = multi_gpu_model(model, GPU_COUNT) #让模型并行训练
```

  ### 5.模型训练
```
#模型训练之前需要先编译。binary_crossentropy是表示模型是二分类任务，如果是多分类则设为为其他参数。
parallel_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy']) 
model_dir = '/data/diyou/data/models/VGG16/' #模型保存目录
log_dir = '/data/diyou/data/logs/' #训练过程中日志保存目录
model_name = model_dir + 'board_EP{epoch:02d}-LOSS{val_loss:.4f}-V6.h5' #模型名称
log_dir = log_dir + datetime.now().strftime('VGG_model_%Y%m%d_%H%M') #训练过程日志名称

#EarlyStopping的作用主要是用于监控模型训练过程中一旦训练效果不满足预期要求可以停止训练。这里设置10个epoch验证损失没用下降则停止训练。
#此时需要进一步调整模型。这里模型任务比较简单，通常二三是个epoch就能拟合了，所以设置为10.如果模型较复杂，可以把该值设大点。
es = EarlyStopping(monitor='val_loss', patience=10)  # 连续10个epoch val_loss没有下降则提前终止

#ModelCheckpoint是用于监控训练过程中，一旦验证损失有所下降，则保存该轮模型权重参数。如果验证损失不下降，则不保存。也就是永远只保存最优的模型。
mc = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True,
                     save_weights_only=True)  # checkpoint的是多GPU的权重或者完整模型
tb = TensorBoard(log_dir=log_dir)#用于可视化

#用于监控训练过程中监控验证损失，如果连续patience个epoch模型验证损失不下降，则降低学习率，降低比例为factor。如果初始学习率设置的比较大，可以减小factor的值。patience的值也可以适当调整。
rrl = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, cooldown=0,
                        verbose=1)  # 连续5个epochs val_loss不下降则降低学习率

# steps_per_epoch:训练数据集样本数/batch_size
# validation_steps:验证数据集样本数/batch_size

#模型开始训练
parallel_model.fit_generator(train_generator, epochs=epochs, validation_data=test_generator,
                             steps_per_epoch=(train_positive_file_count + train_negative_file_count) / batch_size,
                             validation_steps=(test_positive_file_count + test_negative_file_count) / batch_size,
                             callbacks=[es, mc, tb, rrl])

#训练完后保存完整模型
model.save(model_dir + 'board_VGG16_classifier_noparallel_v6.h5')
#保存模型权重参数
model.save_weights(model_dir + 'board_classify_VGG16_noparallel_weights_v6.h5')

import json

#保存模型网络结构
yaml_string = model.to_yaml()
open(model_dir + 'board_noparallel_v6.yaml', 'w').write(yaml_string)
```

  ### 6.模型预测

  使用测试集来评估模型最终的效果。如果模型效果不理想，则需要重新修改模型参数、网络结构、或者数据集。

```
# 模型评价和预测
parallel_model.evaluate_generator(test_generator)
res_arr = parallel_model.predict_generator(test_generator)
```


  


